# Implementation Guide: Freestyle Data Collection Pipeline

## Quick Start

### Prerequisites

```bash
# Required packages
pip install lerobot google-generativeai opencv-python numpy rich fire

# Optional but recommended
pip install wandb  # For experiment tracking
pip install pytest  # For testing
```

### Environment Setup

```bash
# Set up API keys
export GOOGLE_API_KEY="your-gemini-api-key"
export LEROBOT_DATA_DIR="./data"

# Create project structure
mkdir -p freestyle_pipeline/{capture,processing,annotation,generation}
```

## Phase 1: Data Capture Implementation

### Basic Capture Script

```python
# freestyle_pipeline/capture/recorder.py
import cv2
import numpy as np
import time
from pathlib import Path
from datetime import datetime
import json
import h5py
from rich.console import Console
from rich.progress import track

console = Console()

class FreestyleRecorder:
    def __init__(self, output_dir="./recordings", cameras=None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize cameras
        self.cameras = cameras or {
            "side": 0,  # Default webcam
            "wrist": 1  # Secondary camera
        }
        self.cap_objects = {}
        self._initialize_cameras()
        
    def _initialize_cameras(self):
        """Initialize camera capture objects"""
        for name, index in self.cameras.items():
            cap = cv2.VideoCapture(index)
            if not cap.isOpened():
                console.print(f"[red]Warning: Camera '{name}' at index {index} not available[/red]")
            else:
                # Set camera properties
                cap.set(cv2.CAP_PROP_FPS, 30)
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                self.cap_objects[name] = cap
                console.print(f"[green]âœ“ Camera '{name}' initialized[/green]")
    
    def record_session(self, duration_seconds=300, operator_name="anonymous"):
        """Record a freestyle session"""
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_dir = self.output_dir / session_id
        session_dir.mkdir(exist_ok=True)
        
        console.print(f"\n[bold cyan]Starting 5-minute freestyle recording session[/bold cyan]")
        console.print(f"Session ID: {session_id}")
        console.print(f"Operator: {operator_name}")
        console.print("\n[yellow]Instructions:[/yellow]")
        console.print("- Interact naturally with the robot")
        console.print("- Try various tasks: picking, placing, stacking, pushing")
        console.print("- Be creative! The AI will understand what you're doing")
        console.print("\nPress 'q' to stop early, 's' to mark segment boundary\n")
        
        # Initialize video writers
        writers = {}
        fps = 30
        frame_size = (640, 480)
        
        for name, cap in self.cap_objects.items():
            video_path = session_dir / f"{name}_camera.mp4"
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writers[name] = cv2.VideoWriter(
                str(video_path), fourcc, fps, frame_size
            )
        
        # Recording loop
        start_time = time.time()
        frame_count = 0
        segment_markers = []
        
        try:
            while (time.time() - start_time) < duration_seconds:
                frames = {}
                
                # Capture from all cameras
                for name, cap in self.cap_objects.items():
                    ret, frame = cap.read()
                    if ret:
                        frames[name] = frame
                        writers[name].write(frame)
                
                # Display main camera feed
                if "side" in frames:
                    # Add overlay information
                    elapsed = time.time() - start_time
                    cv2.putText(frames["side"], 
                               f"Time: {elapsed:.1f}s | Frame: {frame_count}", 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                               0.7, (0, 255, 0), 2)
                    
                    cv2.imshow("Freestyle Recording - Side View", frames["side"])
                
                # Handle keyboard input
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    console.print("\n[yellow]Recording stopped by user[/yellow]")
                    break
                elif key == ord('s'):
                    segment_markers.append({
                        "frame": frame_count,
                        "time": elapsed
                    })
                    console.print(f"[green]Segment marker added at {elapsed:.1f}s[/green]")
                
                frame_count += 1
                
        finally:
            # Cleanup
            for writer in writers.values():
                writer.release()
            cv2.destroyAllWindows()
        
        # Save metadata
        metadata = {
            "session_id": session_id,
            "operator": operator_name,
            "duration": time.time() - start_time,
            "frame_count": frame_count,
            "fps": fps,
            "cameras": list(self.cameras.keys()),
            "segment_markers": segment_markers,
            "timestamp": datetime.now().isoformat()
        }
        
        with open(session_dir / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        console.print(f"\n[bold green]Recording complete![/bold green]")
        console.print(f"Duration: {metadata['duration']:.1f} seconds")
        console.print(f"Frames captured: {frame_count}")
        console.print(f"Output directory: {session_dir}")
        
        return session_dir

# Example usage
if __name__ == "__main__":
    recorder = FreestyleRecorder()
    session_path = recorder.record_session(duration_seconds=300)
```

## Phase 2: Video Processing Implementation

### Segmentation Preprocessor

```python
# freestyle_pipeline/processing/segmenter.py
import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import json
from rich.progress import track

class VideoSegmenter:
    def __init__(self, motion_threshold=30, min_segment_duration=2.0):
        self.motion_threshold = motion_threshold
        self.min_segment_duration = min_segment_duration
        
    def detect_segments(self, video_path: Path) -> List[Dict]:
        """Detect action segments based on motion analysis"""
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        segments = []
        motion_scores = []
        
        # First pass: calculate motion scores
        prev_frame = None
        frame_idx = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            if prev_frame is not None:
                # Calculate frame difference
                diff = cv2.absdiff(prev_frame, gray)
                motion_score = np.mean(diff)
                motion_scores.append(motion_score)
            
            prev_frame = gray
            frame_idx += 1
        
        cap.release()
        
        # Second pass: identify segments
        segments = self._identify_segments(motion_scores, fps)
        
        return segments
    
    def _identify_segments(self, motion_scores: List[float], fps: float) -> List[Dict]:
        """Identify distinct segments from motion scores"""
        segments = []
        min_frames = int(self.min_segment_duration * fps)
        
        # Smooth motion scores
        window_size = int(fps / 2)  # 0.5 second window
        smoothed = np.convolve(motion_scores, 
                               np.ones(window_size)/window_size, 
                               mode='same')
        
        # Find boundaries (significant changes in motion)
        threshold = np.mean(smoothed) + np.std(smoothed)
        boundaries = [0]
        
        for i in range(1, len(smoothed) - 1):
            if (smoothed[i] > threshold and smoothed[i-1] <= threshold) or \
               (smoothed[i] < threshold and smoothed[i-1] >= threshold):
                if i - boundaries[-1] >= min_frames:
                    boundaries.append(i)
        
        boundaries.append(len(motion_scores))
        
        # Create segments
        for i in range(len(boundaries) - 1):
            start_frame = boundaries[i]
            end_frame = boundaries[i + 1]
            
            segments.append({
                "segment_id": i,
                "start_frame": start_frame,
                "end_frame": end_frame,
                "start_time": start_frame / fps,
                "end_time": end_frame / fps,
                "duration": (end_frame - start_frame) / fps,
                "motion_profile": smoothed[start_frame:end_frame].tolist()
            })
        
        return segments
    
    def extract_segment_clips(self, video_path: Path, segments: List[Dict], 
                            output_dir: Path) -> List[Path]:
        """Extract individual video clips for each segment"""
        output_dir.mkdir(exist_ok=True)
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        segment_paths = []
        
        for segment in track(segments, description="Extracting segments"):
            output_path = output_dir / f"segment_{segment['segment_id']:03d}.mp4"
            
            # Set up video writer
            cap.set(cv2.CAP_PROP_POS_FRAMES, segment['start_frame'])
            ret, frame = cap.read()
            if not ret:
                continue
                
            height, width = frame.shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
            
            # Write segment frames
            for frame_idx in range(segment['start_frame'], segment['end_frame']):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    writer.write(frame)
            
            writer.release()
            segment_paths.append(output_path)
        
        cap.release()
        return segment_paths
```

## Phase 3: AI Annotation Implementation

### Gemini Annotator

```python
# freestyle_pipeline/annotation/gemini_annotator.py
from google import genai
from google.genai import types
import base64
from pathlib import Path
import json
from typing import Dict, List
import cv2
import numpy as np
from rich.console import Console

console = Console()

class GeminiVideoAnnotator:
    def __init__(self, model_name="gemini-2.5-flash-preview-05-20"):
        self.client = genai.Client(api_key=os.environ.get("GOOGLE_API_KEY"))
        self.model_name = model_name
        
    def annotate_segment(self, video_path: Path, segment_info: Dict) -> Dict:
        """Generate detailed annotations for a video segment"""
        console.print(f"[cyan]Annotating segment {segment_info['segment_id']}...[/cyan]")
        
        # Extract key frames for analysis
        frames = self._extract_key_frames(video_path, num_frames=8)
        
        # Build prompt
        prompt = self._build_annotation_prompt(frames, segment_info)
        
        # Get Gemini response
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.7,
                top_p=0.95,
                max_output_tokens=1000,
            )
        )
        
        # Parse response
        annotations = self._parse_annotation_response(response.text)
        annotations.update(segment_info)
        
        return annotations
    
    def _extract_key_frames(self, video_path: Path, num_frames: int = 8) -> List[np.ndarray]:
        """Extract evenly spaced frames from video"""
        cap = cv2.VideoCapture(str(video_path))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
        frames = []
        
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                # Resize for efficiency
                frame = cv2.resize(frame, (320, 240))
                frames.append(frame)
        
        cap.release()
        return frames
    
    def _build_annotation_prompt(self, frames: List[np.ndarray], 
                               segment_info: Dict) -> str:
        """Build prompt for Gemini with video frames"""
        # Convert frames to base64
        frame_data = []
        for i, frame in enumerate(frames):
            _, buffer = cv2.imencode('.jpg', frame)
            b64_frame = base64.b64encode(buffer).decode('utf-8')
            frame_data.append({
                "mime_type": "image/jpeg",
                "data": b64_frame
            })
        
        prompt = f"""Analyze this robot manipulation sequence and provide detailed annotations.

The video segment is {segment_info['duration']:.1f} seconds long.

Based on the frames shown (evenly distributed throughout the segment), provide:

1. **Task Description**: A clear, concise description of what the robot is doing
2. **Subtasks**: Break down the action into 3-5 sequential steps
3. **Objects**: List all objects the robot interacts with
4. **Skills**: Identify the key manipulation skills demonstrated
5. **Quality Assessment**: Rate the execution quality (0-1) and note any issues

Format your response as JSON with these exact keys:
{{
    "task_description": "string",
    "subtasks": [
        {{"description": "string", "start_percent": 0, "end_percent": 25}},
        ...
    ],
    "objects_involved": ["object1", "object2"],
    "skills_demonstrated": ["skill1", "skill2"],
    "quality_score": 0.95,
    "notes": "string"
}}

Analyze the following frames:"""
        
        return [prompt] + frame_data
    
    def _parse_annotation_response(self, response_text: str) -> Dict:
        """Parse JSON response from Gemini"""
        try:
            # Extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                console.print("[red]Failed to parse JSON from response[/red]")
                return self._get_default_annotation()
        except Exception as e:
            console.print(f"[red]Error parsing response: {e}[/red]")
            return self._get_default_annotation()
    
    def _get_default_annotation(self) -> Dict:
        """Return default annotation structure"""
        return {
            "task_description": "Unknown manipulation task",
            "subtasks": [{"description": "Perform action", "start_percent": 0, "end_percent": 100}],
            "objects_involved": ["unknown_object"],
            "skills_demonstrated": ["manipulation"],
            "quality_score": 0.5,
            "notes": "Automatic annotation failed"
        }

    def batch_annotate(self, video_segments: List[Tuple[Path, Dict]], 
                      output_path: Path) -> List[Dict]:
        """Annotate multiple segments and save results"""
        annotations = []
        
        for video_path, segment_info in track(video_segments, 
                                            description="Annotating segments"):
            try:
                annotation = self.annotate_segment(video_path, segment_info)
                annotations.append(annotation)
                
                # Save intermediate results
                with open(output_path, 'w') as f:
                    json.dump(annotations, f, indent=2)
                    
            except Exception as e:
                console.print(f"[red]Error annotating segment {segment_info['segment_id']}: {e}[/red]")
                annotations.append({
                    **segment_info,
                    **self._get_default_annotation()
                })
        
        return annotations
```

## Phase 4: Dataset Generation Implementation

### LeRobot Dataset Builder

```python
# freestyle_pipeline/generation/dataset_builder.py
from pathlib import Path
import json
import shutil
from typing import List, Dict
import numpy as np
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from rich.console import Console
from rich.table import Table

console = Console()

class FreestyleDatasetBuilder:
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def build_from_annotations(self, session_dir: Path, annotations: List[Dict]) -> LeRobotDataset:
        """Build LeRobot dataset from annotated segments"""
        console.print("[bold cyan]Building LeRobot dataset from annotations...[/bold cyan]")
        
        # Create dataset structure
        dataset_name = f"freestyle_{session_dir.name}"
        dataset_path = self.output_dir / dataset_name
        dataset_path.mkdir(exist_ok=True)
        
        # Initialize dataset metadata
        metadata = {
            "dataset_name": dataset_name,
            "robot_type": "unknown",  # To be filled from session metadata
            "fps": 30,
            "features": {
                "observation.images.side": {
                    "shape": [480, 640, 3],
                    "dtype": "uint8"
                },
                "action": {
                    "shape": [7],  # Adjust based on your robot
                    "dtype": "float32"
                }
            },
            "tasks": {},
            "episodes": []
        }
        
        # Process each annotated segment as an episode
        for i, annotation in enumerate(annotations):
            episode_data = self._create_episode(
                session_dir, annotation, episode_index=i
            )
            metadata["episodes"].append(episode_data)
            metadata["tasks"][i] = annotation["task_description"]
        
        # Save metadata
        with open(dataset_path / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        # Copy video files
        self._organize_videos(session_dir, dataset_path, annotations)
        
        # Generate summary statistics
        self._generate_summary(metadata, annotations)
        
        console.print(f"[bold green]Dataset created successfully![/bold green]")
        console.print(f"Location: {dataset_path}")
        console.print(f"Episodes: {len(annotations)}")
        
        return dataset_path
    
    def _create_episode(self, session_dir: Path, annotation: Dict, 
                       episode_index: int) -> Dict:
        """Create episode entry from annotation"""
        return {
            "episode_index": episode_index,
            "task": annotation["task_description"],
            "start_frame": annotation["start_frame"],
            "end_frame": annotation["end_frame"],
            "duration": annotation["duration"],
            "quality_score": annotation.get("quality_score", 1.0),
            "subtasks": annotation.get("subtasks", []),
            "objects": annotation.get("objects_involved", []),
            "skills": annotation.get("skills_demonstrated", []),
            "auto_annotated": True
        }
    
    def _organize_videos(self, session_dir: Path, dataset_path: Path, 
                        annotations: List[Dict]):
        """Organize video files according to LeRobot structure"""
        videos_dir = dataset_path / "videos"
        videos_dir.mkdir(exist_ok=True)
        
        # Copy segmented videos
        for annotation in annotations:
            segment_id = annotation["segment_id"]
            src_video = session_dir / "segments" / f"segment_{segment_id:03d}.mp4"
            
            if src_video.exists():
                dst_video = videos_dir / f"episode_{segment_id:06d}.mp4"
                shutil.copy2(src_video, dst_video)
    
    def _generate_summary(self, metadata: Dict, annotations: List[Dict]):
        """Generate and display dataset summary"""
        table = Table(title="Freestyle Dataset Summary")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")
        
        # Calculate statistics
        total_duration = sum(a["duration"] for a in annotations)
        avg_duration = total_duration / len(annotations) if annotations else 0
        unique_tasks = len(set(a["task_description"] for a in annotations))
        all_skills = []
        for a in annotations:
            all_skills.extend(a.get("skills_demonstrated", []))
        unique_skills = len(set(all_skills))
        
        avg_quality = np.mean([a.get("quality_score", 1.0) for a in annotations])
        
        # Add rows to table
        table.add_row("Total Episodes", str(len(annotations)))
        table.add_row("Total Duration", f"{total_duration:.1f} seconds")
        table.add_row("Average Episode Duration", f"{avg_duration:.1f} seconds")
        table.add_row("Unique Tasks", str(unique_tasks))
        table.add_row("Unique Skills", str(unique_skills))
        table.add_row("Average Quality Score", f"{avg_quality:.2f}")
        
        console.print(table)
        
        # Show task distribution
        console.print("\n[bold]Task Distribution:[/bold]")
        task_counts = {}
        for a in annotations:
            task = a["task_description"]
            task_counts[task] = task_counts.get(task, 0) + 1
        
        for task, count in sorted(task_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            console.print(f"  â€¢ {task}: {count} episodes")
```

## Complete Pipeline Script

### Main Pipeline Orchestrator

```python
# freestyle_pipeline/main.py
import fire
from pathlib import Path
from rich.console import Console
from rich.panel import Panel

from capture.recorder import FreestyleRecorder
from processing.segmenter import VideoSegmenter
from annotation.gemini_annotator import GeminiVideoAnnotator
from generation.dataset_builder import FreestyleDatasetBuilder

console = Console()

class FreestylePipeline:
    def __init__(self):
        self.recorder = FreestyleRecorder()
        self.segmenter = VideoSegmenter()
        self.annotator = GeminiVideoAnnotator()
        
    def record_and_process(self, operator_name: str = "anonymous", 
                          duration: int = 300,
                          output_dir: str = "./datasets"):
        """Complete pipeline: record, segment, annotate, and generate dataset"""
        
        console.print(Panel.fit(
            "[bold cyan]Freestyle Data Collection Pipeline[/bold cyan]\n"
            "Transform 5 minutes of play into hours of annotated data!",
            border_style="cyan"
        ))
        
        # Step 1: Record freestyle session
        console.print("\n[bold]Step 1: Recording Freestyle Session[/bold]")
        session_dir = self.recorder.record_session(
            duration_seconds=duration,
            operator_name=operator_name
        )
        
        # Step 2: Segment the video
        console.print("\n[bold]Step 2: Segmenting Video[/bold]")
        video_path = session_dir / "side_camera.mp4"
        segments = self.segmenter.detect_segments(video_path)
        console.print(f"Found {len(segments)} distinct action segments")
        
        # Extract segment clips
        segments_dir = session_dir / "segments"
        segment_paths = self.segmenter.extract_segment_clips(
            video_path, segments, segments_dir
        )
        
        # Step 3: Annotate segments with AI
        console.print("\n[bold]Step 3: AI Annotation[/bold]")
        video_segments = list(zip(segment_paths, segments))
        annotations_path = session_dir / "annotations.json"
        annotations = self.annotator.batch_annotate(
            video_segments, annotations_path
        )
        
        # Step 4: Generate dataset
        console.print("\n[bold]Step 4: Dataset Generation[/bold]")
        builder = FreestyleDatasetBuilder(Path(output_dir))
        dataset_path = builder.build_from_annotations(session_dir, annotations)
        
        console.print("\n[bold green]Pipeline Complete! ðŸŽ‰[/bold green]")
        console.print(f"Dataset saved to: {dataset_path}")
        
        return dataset_path
    
    def process_existing(self, session_dir: str, output_dir: str = "./datasets"):
        """Process an existing recording session"""
        session_path = Path(session_dir)
        
        if not session_path.exists():
            console.print(f"[red]Session directory not found: {session_path}[/red]")
            return
        
        # Run steps 2-4 on existing recording
        # ... (similar to above but starting from step 2)

if __name__ == "__main__":
    fire.Fire(FreestylePipeline)
```

## Usage Examples

### Basic Usage

```bash
# Record and process a new session
python -m freestyle_pipeline.main record_and_process --operator_name="John Doe"

# Process an existing recording
python -m freestyle_pipeline.main process_existing --session_dir="./recordings/20240615_143022"
```

### Advanced Configuration

```python
# Custom configuration example
from freestyle_pipeline import FreestylePipeline

# Initialize with custom settings
pipeline = FreestylePipeline()
pipeline.segmenter.motion_threshold = 25  # More sensitive segmentation
pipeline.annotator.model_name = "gemini-2.5-pro-preview-03-25"  # Use more powerful model

# Process with custom parameters
dataset = pipeline.record_and_process(
    operator_name="Expert User",
    duration=600,  # 10 minutes
    output_dir="./expert_datasets"
)
```

## Testing and Validation

### Unit Tests

```python
# tests/test_segmentation.py
import pytest
from freestyle_pipeline.processing.segmenter import VideoSegmenter

def test_segment_detection():
    segmenter = VideoSegmenter()
    test_video = "tests/fixtures/sample_video.mp4"
    
    segments = segmenter.detect_segments(test_video)
    
    assert len(segments) > 0
    assert all(s["duration"] >= 2.0 for s in segments)  # Min duration
    assert all(s["end_frame"] > s["start_frame"] for s in segments)
```

### Integration Tests

```python
# tests/test_pipeline.py
def test_full_pipeline():
    pipeline = FreestylePipeline()
    
    # Use test video instead of recording
    test_session = "tests/fixtures/test_session"
    dataset_path = pipeline.process_existing(test_session)
    
    assert dataset_path.exists()
    assert (dataset_path / "metadata.json").exists()
    assert len(list((dataset_path / "videos").glob("*.mp4"))) > 0
```

## Deployment Considerations

### Docker Container

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libopencv-dev \
    python3-opencv \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application
COPY freestyle_pipeline/ ./freestyle_pipeline/

# Set environment variables
ENV PYTHONPATH=/app
ENV LEROBOT_DATA_DIR=/data

# Entry point
ENTRYPOINT ["python", "-m", "freestyle_pipeline.main"]
```

### Cloud Deployment

```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: freestyle-pipeline
spec:
  replicas: 3
  selector:
    matchLabels:
      app: freestyle-pipeline
  template:
    metadata:
      labels:
        app: freestyle-pipeline
    spec:
      containers:
      - name: pipeline
        image: freestyle-pipeline:latest
        env:
        - name: GOOGLE_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-keys
              key: gemini-key
        volumeMounts:
        - name: data-volume
          mountPath: /data
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: pipeline-data-pvc
```

## Next Steps

1. **Implement motor state recording** for robots with accessible APIs
2. **Add multi-camera synchronization** for better 3D understanding
3. **Create web interface** for remote monitoring and control
4. **Implement active learning** to identify segments needing human review
5. **Build evaluation metrics** to compare with traditional datasets

This implementation guide provides a solid foundation for building the freestyle data collection pipeline. The modular design allows for easy extension and customization based on specific robot platforms and requirements. 